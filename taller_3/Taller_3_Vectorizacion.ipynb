{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107d923a",
   "metadata": {},
   "source": [
    "# 1. Limpieza de una noticia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb80136",
   "metadata": {},
   "source": [
    "Busca una noticia o crónica que tenga al menos 2000 palabras y realiza todos los pasos que hemos realizado hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40010498",
   "metadata": {},
   "source": [
    "https://ciencia.nasa.gov/universo/que-es-la-energia-oscura/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac5a8f",
   "metadata": {},
   "source": [
    "https://ciencia.nasa.gov/universo/historia-de-dos-telescopios-wfirst-y-hubble/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4f60",
   "metadata": {},
   "source": [
    "Guillermo Luigui Ubaldo Nieto Angarita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f877a",
   "metadata": {},
   "source": [
    "## 1.1 Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c073521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import stanza\n",
    "import nltk\n",
    "import pdfminer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from pdfminer.high_level import extract_text\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from huggingface_hub import login\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e80da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:43:14,722 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-LOC, S-ORG, B-PER, I-PER, E-PER, S-MISC, B-ORG, E-ORG, S-PER, I-ORG, B-LOC, E-LOC, B-MISC, E-MISC, I-MISC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Introduce tu token aquí\n",
    "token = os.getenv(\"FLAIR_TOKEN\")\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "# Ahora puedes cargar el modelo\n",
    "# The model name has been changed from 'ner-spanish' to 'flair/ner-spanish-large'\n",
    "tagger = SequenceTagger.load('flair/ner-spanish-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b177ec",
   "metadata": {},
   "source": [
    "## 1.2 Descargar los tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8524ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2820dfe78b485eb83b8d83152bfe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:43:16 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-25 12:43:16 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-08-25 12:43:17 INFO: File exists: C:\\Users\\guill\\stanza_resources\\es\\default.zip\n",
      "2025-08-25 12:43:20 INFO: Finished downloading models and saved to C:\\Users\\guill\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb4fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:43:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07abf0465be49458051a984bdfb16cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:43:21 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-25 12:43:22 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-08-25 12:43:22 INFO: Using device: cpu\n",
      "2025-08-25 12:43:22 INFO: Loading: tokenize\n",
      "2025-08-25 12:43:22 INFO: Loading: mwt\n",
      "2025-08-25 12:43:22 INFO: Loading: pos\n",
      "2025-08-25 12:43:23 INFO: Loading: lemma\n",
      "2025-08-25 12:43:24 INFO: Loading: constituency\n",
      "2025-08-25 12:43:24 INFO: Loading: depparse\n",
      "2025-08-25 12:43:25 INFO: Loading: sentiment\n",
      "2025-08-25 12:43:25 INFO: Loading: ner\n",
      "2025-08-25 12:43:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(\"es\")\n",
    "nlp_spacy = spacy.load(\"es_core_news_sm\")  #cargar el modelo en español\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97bbb9",
   "metadata": {},
   "source": [
    "## 1.3 Descargar Stopwords en Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56a2436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spanish_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b6ba8",
   "metadata": {},
   "source": [
    "## 1.4 Cargar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca0dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"texts/que_es_la_energia_oscura_google_chrome_1.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208543ce",
   "metadata": {},
   "source": [
    "## 1.5 Leer PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c93b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    return extract_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aedec",
   "metadata": {},
   "source": [
    "## 1.6 Eliminación de elementos no deseados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095e60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_texts = {\n",
    "    \"1 5   M I N   R E A D\",\n",
    "    \"https://ciencia.nasa.gov/universo/que-es-la-energia-oscura/\",\n",
    "    \"¿Qué es la energía oscura? - NASA Ciencia\",\n",
    "    \"\"\"Equipo de redacción de Ciencia\n",
    "\n",
    "JUN 10, 2024\n",
    "\n",
    "ARTÍCULO\n",
    "\n",
    "ÍNDICE DE CONTENIDOS\n",
    "\n",
    "Una breve historia\n",
    "\n",
    "Todo comenzó con las cefeidas\n",
    "\n",
    "El descubrimiento de un universo en expansión\n",
    "\n",
    "La expansión se está acelerando, según muestran las supernovas\n",
    "\n",
    "¿Qué es exactamente la energía oscura?\n",
    "\n",
    "El futuro\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Compartir\n",
    "\n",
    "Detalles\n",
    "\n",
    "Ú L T I M A   A C T U A L I Z A C I Ó N Mar 27, 2025\n",
    "\n",
    "Términos relacionados\n",
    "\n",
    "Universo\n",
    "\n",
    "Administración Nacional de\n",
    "Aeronáutica y el Espacio\n",
    "\n",
    "La NASA explora lo desconocido en el aire y el\n",
    "\n",
    "espacio, hace innovaciones en beneficio de la\n",
    "\n",
    "humanidad e inspira al mundo con sus\n",
    "\n",
    "descubrimientos.\n",
    "\n",
    "Acerca de la misión de la NASA\n",
    "\n",
    "Join Us\n",
    "\n",
    "Página de\n",
    "inicio\n",
    "\n",
    "Noticias y\n",
    "eventos\n",
    "\n",
    "Multimedia\n",
    "\n",
    "NASA+ L I V E\n",
    "\n",
    "Misiones\n",
    "\n",
    "Seres\n",
    "humanos en el\n",
    "espacio\n",
    "\n",
    "Aeronáutica\n",
    "\n",
    "Tecnología\n",
    "\n",
    "Tierra\n",
    "\n",
    "El sistema\n",
    "solar\n",
    "\n",
    "El universo\n",
    "\n",
    "Ciencia\n",
    "\n",
    "Recursos de\n",
    "aprendizaje\n",
    "\n",
    "Acerca de la\n",
    "NASA\n",
    "\n",
    "NASA en\n",
    "Español\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Sigue a la NASA\n",
    "\n",
    "Mapa del sitio   Para los medios de comunicación\n",
    "\n",
    "Política de privacidad   FOIA   No FEAR Act\n",
    "\n",
    "Otras cuentas sociales de\n",
    "la NASA\n",
    "\n",
    "Oficina del IG   Presupuesto e informes anuales\n",
    "\n",
    "Boletín de la NASA\n",
    "\n",
    "Informes financieros de la agencia   Comunícate con la NASA\n",
    "\n",
    "Accesibilidad\n",
    "\n",
    "Última actualización de la página: Mar 27, 2025\n",
    "\n",
    " Editor/a de esta página: Noelia González\n",
    "\n",
    "Responsible NASA Official for Science: Diana Logreira\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "def extract_text_without_unwanted_elements(text):\n",
    "    for remove_text in remove_texts:\n",
    "        text = re.sub(re.escape(remove_text), \"\", text, flags=re.DOTALL)\n",
    "        text = text.lstrip(\"\\n\")\n",
    "\n",
    "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\s?(AM|PM)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+/\\d+\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581c652",
   "metadata": {},
   "source": [
    "## 1.6 Conversión a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ea6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLowerCase(text):\n",
    "    return text.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6c174",
   "metadata": {},
   "source": [
    "## 1.7 Transformación UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e99c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeTransformation(text, method):\n",
    "    return unicodedata.normalize(method, text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1f20c",
   "metadata": {},
   "source": [
    "## 1.8 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470d4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenization(text):\n",
    "    return [token.text for token in nlp_spacy(text)]\n",
    "\n",
    "def nltk_tokenization(text):\n",
    "    return nltk.word_tokenize(text, language='spanish')\n",
    "\n",
    "def stanza_tokenization(text):\n",
    "    doc = nlp_stanza(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "def bert_tokenization(text):\n",
    "    max_length = 512\n",
    "    tokens_bert = tokenizer_bert.tokenize(text)\n",
    "    chunks = [tokens_bert[i:i+max_length] for i in range(0, len(tokens_bert), max_length)]\n",
    "    tokens_flat = [tok for chunk in chunks for tok in chunk]\n",
    "    return tokens_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab29923",
   "metadata": {},
   "source": [
    "## 1.9 Eliminación de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec9df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function for NLTK ---\n",
    "def remove_stopwords_nltk(text):\n",
    "    \"\"\"\n",
    "    Return Spanish text without stop words using NLTK\n",
    "    \"\"\"\n",
    "    spanish_stopwords = set(stopwords.words(\"spanish\"))\n",
    "    tokens = nltk.word_tokenize(text, language=\"spanish\")\n",
    "    filtered = [word for word in tokens if word.lower() not in spanish_stopwords]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d901d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function for spaCy ---\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"\n",
    "    Return Spanish text without stop words using spaCy\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    filtered = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8ba37",
   "metadata": {},
   "source": [
    "## 1.10 Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b58a6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function 1: Lemmatization with spaCy ---\n",
    "def lemmatize_spanish(text):\n",
    "    \"\"\"\n",
    "    Lemmatize Spanish text (no stopword removal)\n",
    "    \"\"\"\n",
    "    doc = nlp_spacy(text)\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47081e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_flair(text):\n",
    "    \"\"\"Lematizar texto usando Flair\"\"\"\n",
    "    tagger = SequenceTagger.load('pos-fast')\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    lemmas = [token.text for token in sentence.tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb472888",
   "metadata": {},
   "source": [
    "## 1.11 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3d0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function 2: Stemming with NLTK ---\n",
    "def stem_spanish(text):\n",
    "    \"\"\"\n",
    "    Stem Spanish text (no stopword removal)\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    tokens = nltk.word_tokenize(text, language=\"spanish\")\n",
    "    stems = [stemmer.stem(word) for word in tokens if word.isalpha()]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1864ff5",
   "metadata": {},
   "source": [
    "## 1.12 Identificar NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6431b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ner(text):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    entities = [(entity.text, entity.labels[0].value) for entity in sentence.get_spans('ner')]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e151cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(entities):\n",
    "    counter = Counter(entities)\n",
    "    data = [{'Entidad': ent, 'Tipo': typ, 'Frecuencia': freq} for (ent, typ), freq in counter.items()]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee19df",
   "metadata": {},
   "source": [
    "## 1.13 Procedimiento de Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a24d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline():\n",
    "    text = read_pdf(file_path)\n",
    "\n",
    "    text_unicode = unicodeTransformation(text, \"NFKD\")\n",
    "\n",
    "    text_cleaned = extract_text_without_unwanted_elements(text_unicode)\n",
    "\n",
    "    text_lower_case = convertToLowerCase(text_cleaned)\n",
    "\n",
    "    text_without_stop_words = remove_stopwords_spacy(text_lower_case)\n",
    "\n",
    "    tokens = stanza_tokenization(text_without_stop_words)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9234b",
   "metadata": {},
   "source": [
    "## 1.14 Lematización y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2df7ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:43:42,483 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "clean_text = text_processing_pipeline()\n",
    "lemmas = lemmatize_with_flair(clean_text)\n",
    "stems = stem_spanish(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa98f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_with_lemmas = \" \".join(lemmas)\n",
    "clean_text_with_stems = \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ef6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Lema  Frecuencia\n",
      "0       energia          43\n",
      "1      universo          39\n",
      "2        oscura          38\n",
      "3   cientificos          23\n",
      "4      galaxias          18\n",
      "..          ...         ...\n",
      "95    variables           2\n",
      "96     estrella           2\n",
      "97    brillante           2\n",
      "98        tenue           2\n",
      "99        midio           2\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "lemma_counts = Counter(lemmas).most_common(100)\n",
    "df_lemma = pd.DataFrame(lemma_counts, columns=['Lema', 'Frecuencia'])\n",
    "print(df_lemma.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Stems  Frecuencia\n",
      "0    energi          43\n",
      "1   univers          39\n",
      "2     oscur          38\n",
      "3   cientif          27\n",
      "4    galaxi          18\n",
      "..      ...         ...\n",
      "95  humason           3\n",
      "96  respald           3\n",
      "97    fisic           3\n",
      "98     llev           3\n",
      "99    afuer           3\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "stems_counts = Counter(stems).most_common(100)\n",
    "df_stems = pd.DataFrame(stems_counts, columns=['Stems', 'Frecuencia'])\n",
    "print(df_stems.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b04e1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Entidad  Tipo  Frecuencia\n",
      "0           cumulo pandora  MISC           1\n",
      "1                  chandra  MISC           1\n",
      "2    hubble telescopio vlt  MISC           1\n",
      "3        telescopio subaru  MISC           1\n",
      "4               hubble vlt  MISC           1\n",
      "5                 big bang  MISC           1\n",
      "6   henrietta swan leavitt   PER           1\n",
      "7                  leavitt   PER           1\n",
      "8                  slipher   PER           2\n",
      "9                alexander   PER           1\n",
      "10               friedmann   LOC           1\n",
      "11         albert einstein   PER           1\n",
      "12        georges lemaitre   PER           1\n",
      "13               friedmann   PER           1\n",
      "14                einstein   PER           8\n",
      "15                lemaitre   PER           1\n",
      "16            edwin hubble   PER           1\n",
      "17          milton humason   PER           1\n",
      "18                  hubble   PER           2\n",
      "19                 humason   PER           1\n",
      "20              ley hubble  MISC           1\n",
      "21      ley hubblelemaitre  MISC           1\n",
      "22              adam riess   PER           1\n",
      "23         saul perlmutter   PER           1\n",
      "24           brian schmidt   PER           1\n",
      "25           george gamo w   PER           1\n",
      "26     gravedad unimodular  MISC           1\n",
      "27                    nasa   ORG           6\n",
      "28                  euclid  MISC           2\n",
      "29       nancy grace roman   PER           1\n",
      "30                  hubble  MISC           1\n",
      "31        campo vision 100  MISC           1\n",
      "32                  unidos   ORG           1\n",
      "33                   rubin  MISC           1\n",
      "34              james webb   PER           1\n",
      "35                 spherex  MISC           2\n",
      "36            chelsea gohd   ORG           1\n"
     ]
    }
   ],
   "source": [
    "entities = apply_ner(clean_text)\n",
    "df_entities = count_entities(entities)\n",
    "print(df_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970d8cd",
   "metadata": {},
   "source": [
    "## 1.15 Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a644155",
   "metadata": {},
   "source": [
    "### 1.15.1 Vectorización por TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "423bebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e2d9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec_tf_idf.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83086611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10000', '13800', '1912', '1917', '1919', '1922', '1927', '1929',\n",
       "       '1990', '1998', '1a', '2011', '2021', '2023', '2025', '2027',\n",
       "       '2744', '3d', '450', '683', '70', '85', 'abell', 'abril',\n",
       "       'absoluta', 'absolutamente', 'absoluto', 'aceleracion',\n",
       "       'acelerado', 'acelerando', 'acelerar', 'acerca', 'aclarar', 'acto',\n",
       "       'actualidad', 'actualmente', 'adam', 'adicional', 'afirmaba',\n",
       "       'afuera', 'agencia', 'albert', 'aleja', 'alejaban', 'alejado',\n",
       "       'alejan', 'alejando', 'alexander', 'algun', 'ambas', 'anos',\n",
       "       'anteriormente', 'antiguos', 'antiparticulas', 'aparecer',\n",
       "       'apodada', 'apodo', 'apoyan', 'apoyando', 'apoyar', 'arcoiris',\n",
       "       'areas', 'arrugas', 'articulo', 'astronoma', 'astronomicos',\n",
       "       'astronomo', 'astronomos', 'atribuir', 'avances', 'ayudar', 'azul',\n",
       "       'bang', 'basaba', 'belga', 'big', 'brian', 'brillante', 'brillo',\n",
       "       'buscando', 'busqueda', 'cabo', 'calculado', 'cambiado', 'cambio',\n",
       "       'campo', 'campos', 'cancelan', 'cantidad', 'capacidad', 'captar',\n",
       "       'cartografiar', 'causa', 'cefeida', 'cefeidas', 'cerca', 'cercano',\n",
       "       'cero', 'chandra'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_tf_idf.get_feature_names_out()[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98f43808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(1205, 631))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea51819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_idf_array = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26c335",
   "metadata": {},
   "source": [
    "## 1.16 Guardar Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ae337b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/certificado_limpio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c36688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/certificado_limpio_lammes.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b1e45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/certificado_limpio_stems.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fabe493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma.to_excel('lemmas_top100.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0cca342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stems.to_excel('stems_top100.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43275df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.to_excel('entities_top100.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
