{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab6e6dd",
   "metadata": {},
   "source": [
    "# 1. Limpieza del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef73a0",
   "metadata": {},
   "source": [
    "Guillermo Luigui Ubaldo Nieto Angarita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf020178",
   "metadata": {},
   "source": [
    "## 1.1 Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c743bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber  # Para extraer texto de PDFs\n",
    "import re  # Para trabajar con expresiones regulares\n",
    "import spacy  # Para procesamiento de lenguaje natural (NLP)\n",
    "import unicodedata  # Para normalizar caracteres Unicode\n",
    "import stanza\n",
    "import nltk\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbd4a5",
   "metadata": {},
   "source": [
    "## 1.2 Descargar los tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc0d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 6.67MB/s]                    \n",
      "2025-08-15 23:58:58 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-15 23:58:58 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-08-15 23:58:59 INFO: File exists: C:\\Users\\guill\\stanza_resources\\es\\default.zip\n",
      "2025-08-15 23:59:02 INFO: Finished downloading models and saved to C:\\Users\\guill\\stanza_resources\n",
      "2025-08-15 23:59:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 6.14MB/s]                    \n",
      "2025-08-15 23:59:02 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-15 23:59:03 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-08-15 23:59:03 INFO: Using device: cpu\n",
      "2025-08-15 23:59:03 INFO: Loading: tokenize\n",
      "2025-08-15 23:59:03 INFO: Loading: mwt\n",
      "2025-08-15 23:59:03 INFO: Loading: pos\n",
      "2025-08-15 23:59:05 INFO: Loading: lemma\n",
      "2025-08-15 23:59:05 INFO: Loading: constituency\n",
      "2025-08-15 23:59:06 INFO: Loading: depparse\n",
      "2025-08-15 23:59:06 INFO: Loading: sentiment\n",
      "2025-08-15 23:59:06 INFO: Loading: ner\n",
      "2025-08-15 23:59:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stanza.download('es')\n",
    "nlp_stanza = stanza.Pipeline(\"es\")\n",
    "nlp_spacy = spacy.load(\"es_core_news_sm\")  #cargar el modelo en español\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97a968",
   "metadata": {},
   "source": [
    "Descargar y cargar los tokenizadores de NLTK, Stanza, spaCy y BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b0c1f8",
   "metadata": {},
   "source": [
    "## 1.2 Descargar Stopwords en Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6987cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "spanish_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e82bd",
   "metadata": {},
   "source": [
    "## 1.2 Cargar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5751fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"texts/certificado.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac2766",
   "metadata": {},
   "source": [
    "## 1.3 Eliminar Encabezado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1655ae4",
   "metadata": {},
   "source": [
    "### 1.3.1 Utilizar pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_pattern = r\"Cámara de Comercio de Barranquilla\\s*CERTIFICADO DE EXISTENCIA Y REPRESENTACION LEGAL O\\s*DE INSCRIPCION DE DOCUMENTOS\\.\\s*Fecha de expedición:.*?\\nRecibo No\\..*?\\nCODIGO DE VERIFICACIÓN:.*?\\n\"\n",
    "def extract_text_without_header(pdf_path):\n",
    "    extracted_text = []  # Lista donde almacenaremos el texto de cada página\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:  # Abrir el archivo PDF\n",
    "        for page in pdf.pages:  # Iterar por cada página del PDF\n",
    "            text = page.extract_text()  # Extraer texto de la página\n",
    "            if text:  # Verificar si hay texto en la página\n",
    "                text = re.sub(header_pattern, \"\", text, flags=re.DOTALL)  # Eliminar encabezado con regex\n",
    "                print(text)\n",
    "                extracted_text.append(text)  # Guardar el texto limpio en la lista\n",
    "\n",
    "    return \" \".join(extracted_text)  # Unir todas las páginas en un solo texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf2655",
   "metadata": {},
   "source": [
    "### 1.3.2 Utilizar PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae52824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_without_header(pdf_path):\n",
    "    ignore_texts = {\n",
    "        \"Cámara de Comercio de Barranquilla\\nCERTIFICADO DE EXISTENCIA Y REPRESENTACION LEGAL O\\nDE INSCRIPCION DE DOCUMENTOS.\\nFecha de expedición: 24/10/2024 - 13:11:06\\n\",\n",
    "        \"Recibo No. 12264474, Valor: 7,900\\nCODIGO DE VERIFICACIÓN: YK5CB09DFF\\n\",\n",
    "        \"Página 5 de 5\\n\"\n",
    "    }\n",
    "\n",
    "    remove_texts = {\n",
    "        \"MATRICULA NO RENOVADA\",\n",
    "        \"Actualice su registro y evite sanciones\",\n",
    "        \"\"\"------------------------------------------------------------------------------- \n",
    "Verifique  el  contenido  y  confiabilidad  de  este  certificado,  ingresando a\n",
    "www.camarabaq.org.co/  y digite el código, para que visualice la imagen generada\n",
    "al  momento  de  su  expedición.  La  verificación  se  puede realizar de manera\n",
    "ilimitada,  durante  60  días  calendario  contados  a  partir de la fecha de su\n",
    "expedición.                                                                     \n",
    "--------------------------------------------------------------------------------\"\"\",\n",
    "        \"\"\"**********************************************************************\n",
    "*                                                                    *\n",
    "*  ATENCION:. ESTE COMERCIANTE NO HA CUMPLIDO CON SU DEBER LEGAL      *\n",
    "*            DE RENOVAR SU MATRICULA MERCANTIL.                      *\n",
    "*                                                                    *\n",
    "**********************************************************************\"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clean_pages = []\n",
    "    \n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        page_text = []\n",
    "        for b in blocks:\n",
    "            text = b[4]\n",
    "            # Skip if text matches one of the ignore strings\n",
    "            if text in ignore_texts:\n",
    "                continue\n",
    "            page_text.append(text.strip())\n",
    "        clean_pages.append(\"\\n\".join(page_text))\n",
    "\n",
    "    clean_text = \"\\n\".join(clean_pages)\n",
    "\n",
    "    for remove_text in remove_texts:\n",
    "        clean_text = re.sub(re.escape(remove_text), \"\", clean_text, flags=re.DOTALL)\n",
    "        clean_text = clean_text.lstrip(\"\\n\")\n",
    "\n",
    "    \n",
    "    clean_text = re.sub(\"C E R T I F I C A\", \"CERTIFICA\", clean_text, flags=re.DOTALL)\n",
    "    clean_text = re.sub(r\"Página\\s+\\d+\\s+de\\s+\\d+\", \"\", clean_text)\n",
    "    clean_text = re.sub(r'[ ]{2,}', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\n[ \\t]*\\n+', '\\n', clean_text)\n",
    "    #print(clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e74e",
   "metadata": {},
   "source": [
    "Esta función elimina el encabezado, descarta textos no deseados, reformatea el contenido, suprime la numeración de páginas y corrige espacios y saltos de línea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a79408",
   "metadata": {},
   "source": [
    "## 1.4 Conversión a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLowerCase(text):\n",
    "    return text.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c55044",
   "metadata": {},
   "source": [
    "Convertir todo el texto a minúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a9a61",
   "metadata": {},
   "source": [
    "## 1.5 Transformación UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeTransformation(text, method):\n",
    "    return unicodedata.normalize(method, text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb74ac",
   "metadata": {},
   "source": [
    "Normalizar caracteres Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f553a0",
   "metadata": {},
   "source": [
    "## 1.6 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenization(text):\n",
    "    return [token.text for token in nlp_spacy(text)]\n",
    "\n",
    "def nltk_tokenization(text):\n",
    "    return nltk.word_tokenize(text, language='spanish')\n",
    "\n",
    "def stanza_tokenization(text):\n",
    "    doc = nlp_stanza(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "def bert_tokenization(text):\n",
    "    max_length = 512\n",
    "    tokens_bert = tokenizer_bert.tokenize(text)\n",
    "    chunks = [tokens_bert[i:i+max_length] for i in range(0, len(tokens_bert), max_length)]\n",
    "    tokens_flat = [tok for chunk in chunks for tok in chunk]\n",
    "    return tokens_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9264ef5",
   "metadata": {},
   "source": [
    "Aquí se presentan cuatro funciones para tokenizar texto utilizando SpaCy, NLTK, Stanza y BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efbd6f",
   "metadata": {},
   "source": [
    "## 1.7 Eliminación de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa10036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_nltk(tokens):\n",
    "    \"\"\"\n",
    "    Remove Spanish stopwords using NLTK.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list of str): Tokenized text\n",
    "    \n",
    "    Returns:\n",
    "        list of str: Tokens without stopwords\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token.lower() not in spanish_stopwords]\n",
    "\n",
    "def remove_stopwords_spacy(tokens):\n",
    "    \"\"\"\n",
    "    Remove Spanish stopwords using spaCy.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list of str): Tokenized text\n",
    "    \n",
    "    Returns:\n",
    "        list of str: Tokens without stopwords\n",
    "    \"\"\"\n",
    "    doc = nlp_spacy(\" \".join(tokens))  # rejoin tokens to let spaCy process them\n",
    "    return [token.text for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ad001",
   "metadata": {},
   "source": [
    "Aquí se incluyen dos funciones para eliminar stopwords, una con NLTK y otra con SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c694c52",
   "metadata": {},
   "source": [
    "## 1.9 Eliminación de elementos no desados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8348010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_lower = convertToLowerCase(text) \n",
    "\n",
    "    text_unicode = unicodeTransformation(text_lower,  \"NFKD\")\n",
    "    \n",
    "    tokens = stanza_tokenization(text_unicode)\n",
    "\n",
    "    tokens_with_stop_words_removed = remove_stopwords_nltk(tokens)\n",
    "    return tokens_with_stop_words_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae01df4",
   "metadata": {},
   "source": [
    "En este código, primero se toma un texto, se convierte a minúsculas, se aplica la transformación Unicode, se tokeniza con Stanza y, finalmente, se eliminan las stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f20a",
   "metadata": {},
   "source": [
    "## 1.10 Executar el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e58a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = extract_text_without_header(pdf_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbba2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'matricula', 'mercantil', 'proporciona', 'seguridad', 'confianza', 'negocios', '.', 'renueve', 'matricula', 'mercantil', 'mas', 'tardar', '31', 'marzo', '\"', 'fundamento', 'matrícula', 'inscripciones', 'efectuadas', 'registro', 'mercantil', ',', 'cámara', 'comercio', 'certifica', ':', 'certifica', 'nombre', ',', 'identificación', 'domicilio', 'razón', 'social', ':', 'inversiones', '&', 'comercializadora', 'sandoval', 'orozco', 's.a.s.', 'sigla', ':', 'nit', ':', '900.926.396', '-', '0', 'domicilio', 'principal']\n"
     ]
    }
   ],
   "source": [
    " # Extraer texto limpio del PDF\n",
    "tokens = preprocess_text(raw_text)  # Preprocesar el texto con spaCy\n",
    "\n",
    "print(tokens[:50])  # Mostrar los primeros 50 tokens como prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e9bab",
   "metadata": {},
   "source": [
    "## 1.11 Guardar el achievo como un txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "691f97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/certificado_limpio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d43a9b",
   "metadata": {},
   "source": [
    "## 1.12 Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb56e8",
   "metadata": {},
   "source": [
    "### 1.12.1 ¿Se deben eliminar los StopWords? o Solo algunas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8570b",
   "metadata": {},
   "source": [
    "Considero que es mejor eliminarlas ya hay suficiente texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9d234",
   "metadata": {},
   "source": [
    "### 1.12.2 ¿Qué hacemos con los números?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d03436",
   "metadata": {},
   "source": [
    "Apliqué la librería Stanza porque conserva los números de forma intacta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc7b23",
   "metadata": {},
   "source": [
    "### 1.12.3 Investigue si existen otras formas de eliminar los encabezados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9a254",
   "metadata": {},
   "source": [
    "#### 1.12.4 ¿qué librerías existen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815ca43",
   "metadata": {},
   "source": [
    "| Biblioteca                                                                                   | Cómo ayuda a eliminar encabezados                                                                                                                         | Pros                                                                        | Contras                                                                 |\n",
    "| -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ----------------------------------------------------------------------- |\n",
    "| **[pdfplumber](https://github.com/jsvine/pdfplumber)**                                       | Extrae texto con un diseño preciso. Puedes detectar la primera línea de cada página (encabezado) o bloques en la parte superior y excluirlos.             | Fácil para filtrar texto rápidamente. Funciona bien con PDFs estructurados. | No funciona bien con PDFs muy desordenados o escaneados.                |\n",
    "| **[PyMuPDF (fitz)](https://pymupdf.readthedocs.io/)**                                        | Permite extraer texto por coordenadas de caja delimitadora, por lo que puedes omitir el contenido del margen superior donde suelen estar los encabezados. | Alto control sobre posiciones y elementos.                                  | Requiere conocer las coordenadas o patrones del encabezado.             |\n",
    "| **[PDFMiner.six](https://github.com/pdfminer/pdfminer.six)**                                 | Da acceso de bajo nivel a la posición del texto; puedes descartar cualquier texto dentro de ciertos rangos de coordenadas *y*.                            | Muy preciso.                                                                | API más compleja que pdfplumber.                                        |\n",
    "| **[pikepdf](https://github.com/pikepdf/pikepdf)**                                            | Trabaja a nivel de objetos PDF — permite eliminar objetos de encabezado (si están separados).                                                             | Útil para PDFs con una estructura de objetos consistente.                   | No es bueno para detección de texto en línea; requiere conocer objetos. |\n",
    "| **[Camelot](https://camelot-py.readthedocs.io/)** / **[Tabula](https://tabula.technology/)** | Diseñadas para tablas, pero permiten segmentar páginas e ignorar filas de encabezado en los datos extraídos.                                              | Excelentes para PDFs tabulares.                                             | No sirven para documentos de texto general.                             |\n",
    "| **[pdfrw](https://github.com/pmaupin/pdfrw)**                                                | Permite manipular objetos de página y potencialmente eliminar capas de encabezado.                                                                        | Puede editar PDFs directamente.                                             | Bajo nivel, no orientada al texto.                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b772625",
   "metadata": {},
   "source": [
    "#### 1.12.5 ¿Cómo se pueden aplicar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf1b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9edd3992",
   "metadata": {},
   "source": [
    "#### 1.12.6 ¿En qué eventos los podemos implementar?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
