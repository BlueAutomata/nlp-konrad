{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab6e6dd",
   "metadata": {},
   "source": [
    "# 1. Limpieza del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef73a0",
   "metadata": {},
   "source": [
    "Guillermo Luigui Ubaldo Nieto Angarita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf020178",
   "metadata": {},
   "source": [
    "## 1.1 Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c743bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber  # Para extraer texto de PDFs\n",
    "import re  # Para trabajar con expresiones regulares\n",
    "import spacy  # Para procesamiento de lenguaje natural (NLP)\n",
    "import unicodedata  # Para normalizar caracteres Unicode\n",
    "import stanza\n",
    "import nltk\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "afc0d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 5.10MB/s]                    \n",
      "2025-08-15 13:24:59 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-15 13:24:59 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-08-15 13:25:00 INFO: File exists: C:\\Users\\guill\\stanza_resources\\es\\default.zip\n",
      "2025-08-15 13:25:03 INFO: Finished downloading models and saved to C:\\Users\\guill\\stanza_resources\n",
      "2025-08-15 13:25:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 4.45MB/s]                    \n",
      "2025-08-15 13:25:03 INFO: Downloaded file to C:\\Users\\guill\\stanza_resources\\resources.json\n",
      "2025-08-15 13:25:04 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-08-15 13:25:04 INFO: Using device: cpu\n",
      "2025-08-15 13:25:04 INFO: Loading: tokenize\n",
      "2025-08-15 13:25:04 INFO: Loading: mwt\n",
      "2025-08-15 13:25:04 INFO: Loading: pos\n",
      "2025-08-15 13:25:06 INFO: Loading: lemma\n",
      "2025-08-15 13:25:07 INFO: Loading: constituency\n",
      "2025-08-15 13:25:07 INFO: Loading: depparse\n",
      "2025-08-15 13:25:07 INFO: Loading: sentiment\n",
      "2025-08-15 13:25:08 INFO: Loading: ner\n",
      "2025-08-15 13:25:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stanza.download('es')\n",
    "nlp_stanza = stanza.Pipeline(\"es\")\n",
    "nlp_spacy = spacy.load(\"es_core_news_sm\")  #cargar el modelo en español\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e82bd",
   "metadata": {},
   "source": [
    "## 1.2 Cargar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b5751fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"texts/certificado.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac2766",
   "metadata": {},
   "source": [
    "## 1.3 Eliminar Encabezado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1fb124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_pattern = r\"Cámara de Comercio de Barranquilla\\s*CERTIFICADO DE EXISTENCIA Y REPRESENTACION LEGAL O\\s*DE INSCRIPCION DE DOCUMENTOS\\.\\s*Fecha de expedición:.*?\\nRecibo No\\..*?\\nCODIGO DE VERIFICACIÓN:.*?\\n\"\n",
    "def extract_text_without_header(pdf_path):\n",
    "    extracted_text = []  # Lista donde almacenaremos el texto de cada página\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:  # Abrir el archivo PDF\n",
    "        for page in pdf.pages:  # Iterar por cada página del PDF\n",
    "            text = page.extract_text()  # Extraer texto de la página\n",
    "            if text:  # Verificar si hay texto en la página\n",
    "                text = re.sub(header_pattern, \"\", text, flags=re.DOTALL)  # Eliminar encabezado con regex\n",
    "                print(text)\n",
    "                extracted_text.append(text)  # Guardar el texto limpio en la lista\n",
    "\n",
    "    return \" \".join(extracted_text)  # Unir todas las páginas en un solo texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aae52824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_without_header(pdf_path):\n",
    "    ignore_texts = {\n",
    "        \"Cámara de Comercio de Barranquilla\\nCERTIFICADO DE EXISTENCIA Y REPRESENTACION LEGAL O\\nDE INSCRIPCION DE DOCUMENTOS.\\nFecha de expedición: 24/10/2024 - 13:11:06\\n\",\n",
    "        \"Recibo No. 12264474, Valor: 7,900\\nCODIGO DE VERIFICACIÓN: YK5CB09DFF\\n\",\n",
    "        \"Página 5 de 5\\n\"\n",
    "    }\n",
    "\n",
    "    remove_texts = {\n",
    "        \"MATRICULA NO RENOVADA\",\n",
    "        \"Actualice su registro y evite sanciones\",\n",
    "        \"\"\"------------------------------------------------------------------------------- \n",
    "Verifique  el  contenido  y  confiabilidad  de  este  certificado,  ingresando a\n",
    "www.camarabaq.org.co/  y digite el código, para que visualice la imagen generada\n",
    "al  momento  de  su  expedición.  La  verificación  se  puede realizar de manera\n",
    "ilimitada,  durante  60  días  calendario  contados  a  partir de la fecha de su\n",
    "expedición.                                                                     \n",
    "--------------------------------------------------------------------------------\"\"\",\n",
    "        \"\"\"**********************************************************************\n",
    "*                                                                    *\n",
    "*  ATENCION:. ESTE COMERCIANTE NO HA CUMPLIDO CON SU DEBER LEGAL      *\n",
    "*            DE RENOVAR SU MATRICULA MERCANTIL.                      *\n",
    "*                                                                    *\n",
    "**********************************************************************\"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clean_pages = []\n",
    "    \n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        page_text = []\n",
    "        for b in blocks:\n",
    "            text = b[4]\n",
    "            # Skip if text matches one of the ignore strings\n",
    "            if text in ignore_texts:\n",
    "                continue\n",
    "            page_text.append(text.strip())\n",
    "        clean_pages.append(\"\\n\".join(page_text))\n",
    "\n",
    "    clean_text = \"\\n\".join(clean_pages)\n",
    "\n",
    "    for remove_text in remove_texts:\n",
    "        clean_text = re.sub(re.escape(remove_text), \"\", clean_text, flags=re.DOTALL)\n",
    "        clean_text = clean_text.lstrip(\"\\n\")\n",
    "\n",
    "    \n",
    "    clean_text = re.sub(\"C E R T I F I C A\", \"CERTIFICA\", clean_text, flags=re.DOTALL)\n",
    "    clean_text = re.sub(r\"Página\\s+\\d+\\s+de\\s+\\d+\", \"\", clean_text)\n",
    "    clean_text = re.sub(r'[ ]{2,}', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\n[ \\t]*\\n+', '\\n', clean_text)\n",
    "    #print(clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0101a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = extract_text_without_header(pdf_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a79408",
   "metadata": {},
   "source": [
    "## 1.4 Conversión a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "921fd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLowerCase(text):\n",
    "    return text.lower()  # Convertir todo el texto a minúsculas y procesarlo con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a9a61",
   "metadata": {},
   "source": [
    "## 1.5 Transformación UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bc7c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeTransformation(text, method):\n",
    "    return unicodedata.normalize(method, text)  # Normalizar caracteres Unicode (ej. á → a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f553a0",
   "metadata": {},
   "source": [
    "## 1.6 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "464ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenization(text):\n",
    "    return [token.text for token in nlp_spacy(text)]\n",
    "\n",
    "def nltk_tokenization(text):\n",
    "    return nltk.word_tokenize(text, language='spanish')\n",
    "\n",
    "def stanza_tokenization(text):\n",
    "    doc = nlp_stanza(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "def bert_tokenization(text):\n",
    "    return tokenizer_bert.tokenize(text)\n",
    "\n",
    "def takenization(doc):\n",
    "    tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efbd6f",
   "metadata": {},
   "source": [
    "## 1.7 Eliminación de stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf95c7",
   "metadata": {},
   "source": [
    "## 1.8 Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c694c52",
   "metadata": {},
   "source": [
    "## 1.9 Eliminación de elementos no desados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8348010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_lower = convertToLowerCase(text) \n",
    "\n",
    "    text_unicode = unicodeTransformation(text_lower,  \"NFKD\")\n",
    "    \n",
    "    tokens = nltk_tokenization(text_unicode)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae01df4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0251f20a",
   "metadata": {},
   "source": [
    "## 1.10 Executar el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9e58a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = extract_text_without_header(pdf_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fbba2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"''\", 'la', 'matricula', 'mercantil', 'proporciona', 'seguridad', 'y', 'confianza', 'en', 'los', 'negocios', '.', 'renueve', 'su', 'matricula', 'mercantil', 'a', 'mas', 'tardar', 'el', '31', 'de', 'marzo', \"''\", 'con', 'fundamento', 'en', 'la', 'matrícula', 'e', 'inscripciones', 'efectuadas', 'en', 'el', 'registro', 'mercantil', ',', 'la', 'cámara', 'de', 'comercio', 'certifica', ':', 'certifica', 'nombre', ',', 'identificación', 'y', 'domicilio', 'razón']\n"
     ]
    }
   ],
   "source": [
    " # Extraer texto limpio del PDF\n",
    "tokens = preprocess_text(raw_text)  # Preprocesar el texto con spaCy\n",
    "\n",
    "print(tokens[:50])  # Mostrar los primeros 50 tokens como prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e9bab",
   "metadata": {},
   "source": [
    "## 1.11 Guardar el achievo como un txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "691f97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/certificado_limpio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
